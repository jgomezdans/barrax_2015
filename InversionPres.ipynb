{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Inferring the characteristics of the surface from optical data\n",
    "### J GÃ³mez-Dans (NCEO & UCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* RT theory allows us to explain the scattering & absorption of photons\n",
    "* ... by describing the optical properties and structure of the scene\n",
    "* However, we want to find out about the surface **from the data**!\n",
    "* E.g. we want to infer LAI, chlorophyll, ... from reflectrance measurements \n",
    "* The inverse problem...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The inverse problem\n",
    "* An RT model $\\mathcal{H}$ predicts directional reflectance factor, $\\vec{\\rho}_{m}(\\Omega, \\Omega')$\n",
    "    * $\\dots$ as a function of a set of input parameters: LAI, chlorophyll concentration, equivalent water thickness...\n",
    "* $\\mathcal{H}$ combination leaf RT model, canopy RT model and soil RT model\n",
    "    * PROSPECT (Liberty?)\n",
    "    * SAIL (ACRM, Semidiscrete, ...)\n",
    "    * Linear mixture of spectra assuming Lambertian soil (Walthall, Hapke, ...)\n",
    "* For this lecture, we'll refer to $\\mathcal{H}$ as the **observation operator**\n",
    "* Stack input parameters into vector $\\vec{x}$.\n",
    "* Other information e.g. illumination geometry, etc\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\mathbf{x}, I) = \\vec{\\rho}_m(\\Omega, \\Omega')\n",
    "$$\n",
    "\n",
    "* Our task is to infer $\\vec{x}$ given observations $\\vec{\\rho}(\\Omega, \\Omega')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The model couples the observations and our parameters\n",
    "* In some cases, we might be able to provide an *analytic inversion*\n",
    "* However, we have ignored observational uncertainties\n",
    "* We have also ignored the model uncertainty (*inadequacy*): a model *is not* reality\n",
    "* These uncertainties will translate into uncertainty into our inference of $\\vec{x}$\n",
    "* Is there a framework for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Least squares: minimise data-model mismatch\n",
    "* Assuming observational noise, \n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\mathbf{x}, I) = \\vec{R}_m(\\Omega, \\Omega')\n",
    "$$\n",
    "\n",
    "* Any solution that falls within the error bars cannot be discarded\n",
    "\n",
    "\n",
    "$$\n",
    " \\|\\mathcal{H}(\\vec{x}, I) - \\vec{R}\\|^2 \\le \\epsilon_{obs}\n",
    "$$\n",
    "\n",
    "* Moreover, models very linear\n",
    "* Haven't even considered model inadequacy!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Eg simulate some healthy wheat canopy (LAI=4.2), and then run our RT model with random inputs\n",
    "* Accept solutions that fall within the observational error... ($\\Rightarrow$ *brute force montecarlo!*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Synthetic MC example](./figs/synthetic_inversion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Large uncertainty** in estimate of state.\n",
    "* $\\Rightarrow$ limited information content in the data.\n",
    "* Need more information..\n",
    "    * Add more observations\n",
    "    * Decrease observational uncertainty\n",
    "* Need to consider **sensitivity** of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###Reverend Bayes to the rescue\n",
    "\n",
    "<img src=\"http://rlv.zcache.com/reverend_thomas_bayes_coffee_mug-r832cba30bb8b4a73a6ed6dca65081329_x7jsg_8byvr_512.jpg\" width=\"30%\" height=\"15%\" />\n",
    "\n",
    "* We assume that parameter uncertainty can be encoded if we treat $\\vec{x}$ as a **probability density function** (pdf), $p(\\vec{x})$.\n",
    "    * $p(\\vec{x})$ encodes our belief in the value of $\\vec{x}$\n",
    "    * Natual treatment of uncertainty\n",
    "* We are interested in learning about $p(\\vec{x})$ **conditional** on the observations $\\vec{R}$, $p(\\vec{x}|\\vec{R})$.\n",
    "* **Bayes' Rule** states how we can *learn* about $p(\\vec{x}|\\vec{R})$\n",
    "* In essence, Bayes' rule is a statement on how to *update our beliefs* on $\\vec{x}$ when new *evidence* crops up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "p(\\vec{x} | \\vec{R}, I ) =\\frac{ p (\\vec{R} | \\vec{x}, I)\\cdot p(\\vec{x},I)}{p(\\vec{R})}\\propto p (\\vec{R} | \\vec{x}, I)\\cdot p(\\vec{x},I) \n",
    "$$\n",
    "\n",
    "* $p(\\vec{R}|\\vec{x},I)$ is the **likelihood function**\n",
    "    * encodes the probability of $\\vec{R}$ **given** $\\vec{x}$, and any other information ($I$)\n",
    "* $p(\\vec{x})$ is our *a priori* belief in the pdf of $\\vec{x}$\n",
    "* $p(\\vec{R}$ can be thought of as normalisation constant, and we'll typically ignore it\n",
    "* A way to picture Bayes' rule:\n",
    "\n",
    "$$\n",
    "        p(\\textsf{Hypothesis} | \\textsf{Data},I) \\propto p(\\textsf{Data} | \\textsf{Hypothesis},I) \\times p(\\textsf{Hypothesis} | I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The prior $p(\\vec{x})$\n",
    "\n",
    "* Encodes **everything we know** about $\\vec{x}$ before we even look at the data\n",
    "* In some cases, we can have *uninformative priors*...\n",
    "* ... but the real power is that it allows us to bring understanding, however weak to the problem!\n",
    "\n",
    "## The likelihood $p(\\vec{R}|\\vec{x})$\n",
    "\n",
    "* The likelihood states is our data generative model\n",
    "* It links the experimental results with the quantity of inference\n",
    "* It includes our observations, their uncertainties, but also the model and its uncertainties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Assume that our model is able perfect (*ahem!*), so if $\\vec{x}$ is the **true** state, the model will predict the observation **exactly**\n",
    "* Any disagreement *has to be* due to experimental error. We'll assume it's **additive**:\n",
    "\n",
    "$$\n",
    "\\vec{R}=\\mathcal{H}(\\vec{x}) + \\epsilon\n",
    "$$\n",
    "\n",
    "* Assume that $\\epsilon \\sim \\mathcal{N}(\\vec{\\mu},\\mathbf{\\Sigma}_{obs})$\n",
    "    * For simplicity, $\\vec{\\mu}=\\vec{0}$\n",
    "* The likelihood is then given by\n",
    "$$\n",
    "p(\\vec{R}|\\vec{x})\\propto\\exp\\left[-\\frac{1}{2}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)^{\\top}\\mathbf{\\Sigma}_{obs}^{-1}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)\\right]\n",
    "$$\n",
    "\n",
    "* *Have you ever seen this function?*\n",
    "    * Maybe its univariate cousin...\n",
    "* *Can you say something about (i) the shape of the function and (ii) interesting points?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###The posterior\n",
    "\n",
    "* We can simply multiply the likelihood and prior to obtain the posterior\n",
    "    * In most practical applications with a non-linear $\\mathcal{H}$, there is no closed solution\n",
    "    * However, if $\\mathcal{H}$ is **linear** ($\\mathbf{H}$), we can solve directly\n",
    "* Simple 1D case, assuming Gaussian likelihood & prior:\n",
    "![Simple 1d case](./figs/convert_1dcase.png)\n",
    "* *What can you say about the information content of the data in this synthetic example?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some simple 1D Gaussian maths...\n",
    "\n",
    "* Try to infer $x$ from $y$, using an identity observation operator (i.e., $x=y$, so $\\mathcal(x)=1$ and Gaussian noise:\n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{obs}}\\exp\\left[-\\frac{1}{2}\\frac{(x-y)^2}{\\sigma_{obs}^2}\\right]. \\qquad\\text{Likelihood}\n",
    "$$\n",
    "* Assume that we only know that $x$ is Gaussian distributed with $\\mu_p$ and $\\sigma_0$:\n",
    "\n",
    "$$\n",
    " p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_0^2}\\right]\\qquad\\text{Prior}\n",
    "$$\n",
    "$$\n",
    "p(x|y) \\propto \\frac{1}{\\sigma_{obs}\\sigma_0}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_0^2}\\right]\\cdot \\exp\\left[-\\frac{1}{2}\\frac{(x-y)^2}{\\sigma_{obs}^2}\\right].\\qquad\\text{Posterior}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* The posterior distribution is indeed a Gaussian, and its mean and std dev can be expressed as an **update on the prior values**\n",
    "    * Weighted by the relative weighting of the **uncertainties**\n",
    "\n",
    "$$\n",
    "\\mu_p = \\mu_0 + \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_{obs}^2}(y - \\mu_0).\n",
    "$$\n",
    "$$\n",
    "  \\sigma_p^2 = \\sigma_0^2\\cdot \\left( 1- \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_{obs}^2} \\right)\n",
    "$$\n",
    "\n",
    "* If we now had a new measurement made available, we could use the **posterior** as the **prior**, and it would get updated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ill posed problems\n",
    "\n",
    "* Stepping back from Bayes, consider the logarithm of the likelihood function:\n",
    "\n",
    "$$\n",
    "\\log_{e}\\left(p(\\vec{R}|\\vec{x})\\right)\\propto-\\frac{1}{2}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)^{\\top}\\mathbf{\\Sigma}_{obs}^{-1}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)\n",
    "$$\n",
    "* Maximum occurs when the model matches the observatios\n",
    "* So we can just maximise the model-data mismatch and be done, right?\n",
    "* Remember our generative model, and think what this implies:\n",
    "\n",
    "$$\n",
    " \\|\\mathcal{H}(\\vec{x}, I) - \\vec{R}\\|^2 \\le \\epsilon_{obs}\n",
    "$$\n",
    "\n",
    "* Formally, any solution that falls within the error bars is a reasonable solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Priors\n",
    "\n",
    "* Simplest prior constraints might encode\n",
    "    * range of parameters\n",
    "    * physical limits of parameters (e.g. a mass must be $>0$)\n",
    "* More sophisticated priors might encode more subtle information such as\n",
    "    * expected values from an expert assessment\n",
    "    * expected values from a climatology\n",
    "* It is usually hard to encode the prior information as a pdf\n",
    "    * We tend to use Gaussians as easy to parameterise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###Variational solution\n",
    "\n",
    "* If prior(s) & likelihood **Gaussian**...\n",
    "* And if all terms are **linear**\n",
    "* $\\Rightarrow$ can solve for posterior by minimising $J(\\vec{x})$:\n",
    "\n",
    "$$\n",
    "J(\\vec{x})= \\overbrace{\\frac{1}{2}\\left[\\vec{x}-\\vec{\\mu_{x}}\\right]^{\\top}\\mathbf{C}_{prior}^{-1}\\left[\\vec{x}-\\vec{\\mu_{x}}\\right]}^{\\textrm{Prior}} + \\underbrace{\\frac{1}{2}\\left[\\vec{R}-\\mathcal{H}(\\vec{x})\\right]^{\\top}\\mathbf{C}_{obs}^{-1}\\left[\\vec{R}-\\mathcal{H}(\\vec{x})\\right]}^{\\textrm{Observation}} + \\cdots\n",
    "$$\n",
    "\n",
    "* You can add more constraints (more observations, more prior constraints...)\n",
    "* Uncertainty given by inverse of Hessian @ minimum of $J(\\vec{x})$\n",
    "    * Matrix of second derivatives\n",
    "* Also works OK for weakly non-linear systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###More subtle priors\n",
    "* What about the expectation of \n",
    "    * spatial correlation of the state\n",
    "    * temporal correlation of the state?\n",
    "* Can encode **expectation of smoothness** in the prior\n",
    "* What if we have a model of e.g. LAI evolution with e.g. thermal time?\n",
    "* ... Or a full-blown vegetation/crop model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The concept of a model\n",
    "\n",
    "* Smoothness constraint is a simple vegetation model\n",
    "    * LAI today = LAI tomorrow\n",
    "* This temporal evolution model is **WRONG**\n",
    "* More complex mode likely to also be wrong $\\Rightarrow$ uncertainty!\n",
    "* Encode \"closeness to model\" $\\mathcal{M}(\\vec{x})$ constraint as a Gaussian pdf:\n",
    "$$\n",
    "p(\\vec{x}|\\mathcal{M},I_m)\\propto -\\frac{1}{2}\\exp\\left[ - (\\vec{x} - \\mathcal{M}(I_m)^{\\top}\\mathbf{C}_{model}^{-1}(\\vec{x} - \\mathcal{M}(I_m)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplest model\n",
    "\n",
    "* Smoothness\n",
    "$$\n",
    "x_{k+1} = x_{k} + \\mathcal{N}(0,\\sigma_{model})\n",
    "$$\n",
    "* We can encode this as a matrix, if e.g. we stack $x$ over time in a vector...:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 &-1& 0 & 0 & \\cdots\\\\\n",
    "0 &1 &-1 & 0 & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\cdots \\\\\n",
    "\\end{pmatrix}\\vec{x}\n",
    "$$\n",
    "\n",
    "* It's a linear form!\n",
    "* Model can also be applied in space ($\\Rightarrow$) Markov Random Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###Spatial example\n",
    "\n",
    "* Agricultural irrigation area near CÃ³rdoba\n",
    "* Landsat TM NDVI map\n",
    "\n",
    "![riegos](./figs/riegos_clean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Vertical (red) and horizontal (blue) differences between neighbouring pixels\n",
    "* Over a given threshold\n",
    "![edges](./figs/riegos_edges2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./figs/edge_histo2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ``eoldas_ng``\n",
    "\n",
    "* Python tool that allows you to build variational EO problems\n",
    "* In development\n",
    "* Main bottleneck: slow RT models\n",
    "    * Emulation of models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final remarks\n",
    "* Inversion of a model to infer input parameters is **ill posed**\n",
    "* You need to add more information\n",
    "    * priors\n",
    "    * more observations\n",
    "* Track all the uncertainties\n",
    "* Physically consistent way of merging observations\n",
    "* Variational solution can be efficient\n",
    "* Apply smoothness and model constraints as priors\n",
    "* ``eoldas_ng`` tool"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
