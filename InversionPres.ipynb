{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Inferring the charactetistics of the surface from optical data\n",
    "### J GÃ³mez-Dans (NCEO & UCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* RT theory allows us to explain the scattering & absorption of photons\n",
    "* ... by describing the optical properties and structure of the scene\n",
    "* However, we want to find out about the surface **from the data**!\n",
    "* E.g. we want to infer LAI, chlorophyll, ... from reflectrance measurements \n",
    "* The inverse problem...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The inverse problem\n",
    "* An RT model $\\mathcal{H}$ predicts directional reflectance factor, $\\vec{\\rho}_{m}(\\Omega, \\Omega')$\n",
    "    * \\dots as a function of a set of input parameters: LAI, chlorophyll concentration, equivalent leaf thickness...\n",
    "* Examples of $\\mathcal{H}$ are combinations of a leaf RT model, a canopy RT model and a soil RT model. Eg\n",
    "    * PROSPECT (Liberty?)\n",
    "    * SAIL (ACRM, Semidiscrete, ...)\n",
    "    * Linear mixture of spectra assuming Lambertian soil (Walthall, Hapke, ...)\n",
    "* We typically stack all input parameters into a vector $\\vec{x}$.\n",
    "* We also have other information available (e.g. illumination geometry, etc)\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\mathbf{x}, I) = \\vec{\\rho}_m(\\Omega, \\Omega')\n",
    "$$\n",
    "\n",
    "* Our task is to infer $\\vec{x}$ given observations $\\vec{\\rho}(\\Omega, \\Omega')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The model couples the observations and our parameters\n",
    "* In some cases, we might be able to provide an *analytic inversion*\n",
    "* However, we have ignored observational uncertainties\n",
    "* We have also ignored the model uncertainty (*inadequacy*): a model *is not* reality\n",
    "* These uncertainties will translate into uncertainty into our inference of $\\vec{x}$\n",
    "* Is there a framework for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###Reverend Bayes to the rescue\n",
    "\n",
    "<img src=\"http://rlv.zcache.com/reverend_thomas_bayes_coffee_mug-r832cba30bb8b4a73a6ed6dca65081329_x7jsg_8byvr_512.jpg\" width=\"40%\" height=\"20%\" /><img src=\"http://portrait.kaar.at/Naturwissenschaftler/images/pierre_simon_laplace.jpg\" width=\"45%\"  height=\"20%\"/>\n",
    "\n",
    "* We assume that parameter uncertainty can be encoded if we treat $\\vec{x}$ as a **probability density function** (pdf), $p(\\vec{x})$.\n",
    "* We are interested in learning about $p(\\vec{x})$ **conditional** on the observations $\\vec{R}$, $p(\\vec{x}|\\vec{R})$.\n",
    "* **Bayes' Rule** states how we can *learn* about $p(\\vec{x}|\\vec{R})$\n",
    "* In essence, Bayes' rule is a statement on how to *update our beliefs* on $\\vec{x}$ when new *evidence* crops up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "p(\\vec{x} | \\vec{R}, I ) =\\frac{ p (\\vec{R} | \\vec{x}, I)\\cdot p(\\vec{x},I)}{p(\\vec{R})}\\propto p (\\vec{R} | \\vec{x}, I)\\cdot p(\\vec{x},I) \n",
    "$$\n",
    "\n",
    "* $p(\\vec{R}|\\vec{x},I)$ is the **likelihood function**\n",
    "    * encodes the probability of $\\vec{R}$ **given** $\\vec{x}$, and any other information ($I$)\n",
    "* $p(\\vec{x})$ is our *a priori* belief in the pdf of $\\vec{x}$\n",
    "* $p(\\vec{R}$ can be thought of as normalisation constant, and we'll typically ignore it\n",
    "* A way to picture Bayes' rule:\n",
    "\n",
    "$$\n",
    "        p(\\textsf{Hypothesis} | \\textsf{Data},I) \\propto p(\\textsf{Data} | \\textsf{Hypothesis},I) \\times p(\\textsf{Hypothesis} | I)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The prior $p(\\vec{x})$\n",
    "\n",
    "* Encodes **everything we know** about $\\vec{x}$ before we even look at the data\n",
    "* In some cases, we can have *uninformative priors*...\n",
    "* ... but the real power is that it allows us to bring understanding, however weak to the problem!\n",
    "\n",
    "## The likelihood $p(\\vec{R}|\\vec{x})$\n",
    "\n",
    "* The likelihood states is our data generative model\n",
    "* It links the experimental results with the quantity of inference\n",
    "* It includes our observations, their uncertainties, but also the model and its uncertainties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Assume that our model is able perfect (*ahem!*), so if $\\vec{x}$ is the **true** state, the model will predict the observation **exactly**\n",
    "* Any disagreement *has to be* due to experimental error. We'll assume it's **additive**:\n",
    "\n",
    "$$\n",
    "\\vec{R}=\\mathcal{H}(\\vec{x}) + \\epsilon\n",
    "$$\n",
    "\n",
    "* Assume that $\\epsilon \\sim \\mathcal{N}(\\vec{\\mu},\\mathbf{\\Sigma}_{obs})$\n",
    "    * For simplicity, $\\vec{\\mu}=\\vec{0}$\n",
    "* The likelihood is then given by\n",
    "$$\n",
    "p(\\vec{R}|\\vec{x})\\propto\\exp\\left[-\\frac{1}{2}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)^{\\top}\\mathbf{\\Sigma}_{obs}^{-1}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)\\right]\n",
    "$$\n",
    "\n",
    "* *Have you ever seen this function?*\n",
    "    * Maybe its univariate cousin...\n",
    "* *Can you say something about (i) the shape of the function and (ii) interesting points?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###The posterior\n",
    "\n",
    "* We can simply multiply the likelihood and prior to obtain the posterior\n",
    "    * In most practical applications with a non-linear $\\mathcal{H}$, there is no closed solution\n",
    "    * However, if $\\mathcal{H}$ is **linear** ($\\mathbf{H}$), we can solve directly\n",
    "* Simple 1D case, assuming Gaussian likelihood & prior:\n",
    "![Simple 1d case](./figs/convert_1dcase.png)\n",
    "* *What can you say about the information content of the data in this synthetic example?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some simple 1D Gaussian maths...\n",
    "\n",
    "* Try to infer $x$ from $y$, using an identity observation operator (i.e., $x=y$, so $\\mathcal(x)=1$ and Gaussian noise:\n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{obs}}\\exp\\left[-\\frac{1}{2}\\frac{(x-y)^2}{\\sigma_{obs}^2}\\right]. \\qquad\\text{Likelihood}\n",
    "$$\n",
    "* Assume that we only know that $x$ is Gaussian distributed with $\\mu_p$ and $\\sigma_0$:\n",
    "\n",
    "$$\n",
    " p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_0^2}\\right]\\qquad\\text{Prior}\n",
    "$$\n",
    "$$\n",
    "p(x|y) \\propto \\frac{1}{\\sigma_{obs}\\sigma_0}\\exp\\left[-\\frac{1}{2}\\frac{(x-\\mu_p)^2}{\\sigma_0^2}\\right]\\cdot \\exp\\left[-\\frac{1}{2}\\frac{(x-y)^2}{\\sigma_{obs}^2}\\right].\\qquad\\text{Posterior}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* The posterior distribution is indeed a Gaussian, and its mean and std dev can be expressed as an **update on the prior values**\n",
    "    * Weighted by the relative weighting of the **uncertainties**\n",
    "\n",
    "$$\n",
    "\\mu_p = \\mu_0 + \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_{obs}^2}(y - \\mu_0).\n",
    "$$\n",
    "$$\n",
    "  \\sigma_p^2 = \\sigma_0^2\\cdot \\left( 1- \\frac{\\sigma_0^2}{\\sigma_0^2 + \\sigma_{obs}^2} \\right)\n",
    "$$\n",
    "\n",
    "* If we now had a new measurement made available, we could use the **posterior** as the **prior**, and it would get updated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ill posed problems\n",
    "\n",
    "* Stepping back from Bayes, consider the logarithm of the likelihood function:\n",
    "\n",
    "$$\n",
    "\\log_{e}\\left(p(\\vec{R}|\\vec{x})\\right)\\propto-\\frac{1}{2}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)^{\\top}\\mathbf{\\Sigma}_{obs}^{-1}\\left(\\vec{R}-\\mathcal{H}(\\vec{x})\\right)\n",
    "$$\n",
    "* Maximum occurs when the model matches the observatios\n",
    "* So we can just maximise the model-data mismatch and be done, right?\n",
    "* Remember our generative model, and think what this implies:\n",
    "\n",
    "$$\n",
    " \\|\\mathcal{H}(\\vec{x}, I) - \\vec{R}\\|^2 \\le \\epsilon_{obs}\n",
    "$$\n",
    "\n",
    "* Formally, any solution that falls within the error bars is a reasonable solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Eg simulate some healthy wheat canopy (LAI=4.2), and then run our RT model with random inputs\n",
    "* Accept solutions that fall within the observational error...\n",
    "![Synthetic MC example](./figs/synthetic_inversion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The previous experiment demonstrates that there is a **large uncertainty**.\n",
    "* Effectively, there's limited information content in the data\n",
    "* To solve this, we could add more observations and reduce observational uncertainty\n",
    "    * or add more bands\n",
    "* However, remember that obs will need to be independent!\n",
    "* Ultimately, we need **more information**\n",
    "    * $\\Rightarrow$ Bayes' Rule $\\Rightarrow$ Add **prior information**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
